{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab Experiment 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "docs = np.array(['We are the world we are the children',\n",
        " 'Since the dawn of man is really not that long as every galaxy was formed in less time than it takes to sing this song',\n",
        " 'There was a fire in the room and all the children burnt to barbeque'])"
      ],
      "metadata": {
        "id": "YXY7LHMpQTXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag = vectorizer.fit_transform(docs)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_)\n",
        "print(bag.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld28BubHSOzW",
        "outputId": "7643a242-1472-4ccf-876e-3685beafea84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all', 'and', 'are', 'as', 'barbeque', 'burnt', 'children', 'dawn', 'every', 'fire', 'formed', 'galaxy', 'in', 'is', 'it', 'less', 'long', 'man', 'not', 'of', 'really', 'room', 'since', 'sing', 'song', 'takes', 'than', 'that', 'the', 'there', 'this', 'time', 'to', 'was', 'we', 'world']\n",
            "{'we': 34, 'are': 2, 'the': 28, 'world': 35, 'children': 6, 'since': 22, 'dawn': 7, 'of': 19, 'man': 17, 'is': 13, 'really': 20, 'not': 18, 'that': 27, 'long': 16, 'as': 3, 'every': 8, 'galaxy': 11, 'was': 33, 'formed': 10, 'in': 12, 'less': 15, 'time': 31, 'than': 26, 'it': 14, 'takes': 25, 'to': 32, 'sing': 23, 'this': 30, 'song': 24, 'there': 29, 'fire': 9, 'room': 21, 'and': 1, 'all': 0, 'burnt': 5, 'barbeque': 4}\n",
            "[[0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 1]\n",
            " [0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0]\n",
            " [1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 1 0 0 1 1 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens):\n",
        "    vector=[]\n",
        "    for w in filtered_vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector\n",
        "def unique(sequence):\n",
        "    seen = set()\n",
        "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
        "stopwords=[\"to\",\"is\",\"a\"];special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
        "string1=\"Fire broke out in the room\";string2=\"All the children died\"\n",
        "string1=string1.lower();string2=string2.lower()\n",
        "tokens1=string1.split();tokens2=string2.split()\n",
        "print(tokens1)\n",
        "print(tokens2)\n",
        "#create a vocabulary list\n",
        "vocab=unique(tokens1+tokens2)\n",
        "print(vocab)\n",
        "#filter the vocabulary list\n",
        "filtered_vocab=[]\n",
        "for w in vocab: \n",
        "    if w not in stopwords and w not in special_char: \n",
        "        filtered_vocab.append(w)\n",
        "print(filtered_vocab)\n",
        "#convert sentences into vectords\n",
        "vector1=vectorize(tokens1)\n",
        "print(vector1)\n",
        "vector2=vectorize(tokens2)\n",
        "print(vector2)"
      ],
      "metadata": {
        "id": "1IYU8lAiSW2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c537186-cdb7-4b5e-abb3-531cb3ed325d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fire', 'broke', 'out', 'in', 'the', 'room']\n",
            "['all', 'the', 'children', 'died']\n",
            "['fire', 'broke', 'out', 'in', 'the', 'room', 'all', 'children', 'died']\n",
            "['fire', 'broke', 'out', 'in', 'the', 'room', 'all', 'children', 'died']\n",
            "[1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e3AiRVjYcHrx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}